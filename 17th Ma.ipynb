{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220e5a8-5f7e-44ba-ab95-37b5377ff7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Ans.\n",
    "\n",
    "Missing values are those values in a dataset that are absent or unknown. \n",
    "These values can occur due to various reasons such as human errors, data corruption,\n",
    "or simply missing information.\n",
    "\n",
    "There are several methods to handle missing values, such as imputation, deletion, or prediction. \n",
    "Imputation involves filling in the missing values with an estimated value based on the other observations\n",
    "in the dataset. Deletion can be done by removing the rows or columns containing missing values, but this\n",
    "can lead to a loss of valuable information. Prediction involves using machine learning algorithms to predict \n",
    "the missing values based on the available data.\n",
    "\n",
    "Some algorithms that are not affected by missing values in feature engineering are decision trees, \n",
    "random forests, and support vector machines (SVMs). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83080e-70a8-465b-b790-1bcdb9ce0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Ans.\n",
    "\n",
    "Some Techniques use to handle missing data:\n",
    "    \n",
    "Deletion: This involves deleting the rows or columns containing missing values. \n",
    "However, this approach should be used with caution, as it may lead to a loss of information.\n",
    "\n",
    "Imputation: This involves filling in the missing values with estimates. \n",
    "Some popular methods for imputation include mean imputation, median imputation, mode \n",
    "imputation, and regression imputation.\n",
    "\n",
    "Encoding: In some cases, missing data may be encoded as a separate category, which can be \n",
    "treated as a feature in its own right.\n",
    "\n",
    "   Example with python code are:\n",
    "      \n",
    "    Deletion:\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "data = {'Name': ['Jo', 'sk', 'Ma', 'Ja', 'Da'],\n",
    "        'Age': [25, 30, 20, None, 35],\n",
    "        'Salary': [50000, 60000, None, 70000, 80000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "print(df)\n",
    "\n",
    "    Imputation:\n",
    "        \n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "data = {'Name': ['Jo', 'sk', 'Ma', 'Ja', 'Da'],\n",
    "        'Age': [25, 30, 20, None, 35],\n",
    "        'Salary': [50000, 60000, None, 70000, 80000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['Age'] = imputer.fit_transform(df[['Age']])\n",
    "df['Salary'] = imputer.fit_transform(df[['Salary']])\n",
    "print(df)\n",
    "\n",
    "\n",
    "    Encoding:\n",
    "        \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = {'Name': ['Jo', 'sk', 'Ma', 'Ja', 'Da'],\n",
    "        'Gender': ['Male', 'Male', 'Female', 'Male', 'Male']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Gender'] = le.fit_transform(df['Gender'])\n",
    "print(df)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d75e2c-0c72-46ad-9d3c-379413d5b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Ans.\n",
    "\n",
    "Imbalanced data refers to a situation where the number of samples in each class of a\n",
    "classification problem is not evenly distributed. This means that one or more classes \n",
    "have significantly fewer samples compared to the other classes.\n",
    "\n",
    "If imbalanced data is not handled it can have a \n",
    "negative impact on the performance of machine learning algorithms. This is \n",
    "because most algorithms are designed to maximize overall accuracy and may be biased towards the majority \n",
    "class, resulting in poor predictive performance for the minority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde57442-596d-4372-960e-379f8351d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Ans.\n",
    "\n",
    "Up-sampling and down-sampling are techniques used in machine learning to handle\n",
    "imbalanced data by adjusting the class distribution of a dataset.\n",
    "\n",
    "Up-sampling involves increasing the number of samples in the minority class to balance the class distribution.\n",
    "\n",
    "Down-sampling involves reducing the number of samples in the majority class to balance the class distribution.\n",
    "\n",
    "Here's an example of when up-sampling and down-sampling :\n",
    "\n",
    "Suppose we have a dataset of 1000 customer transactions, where 900 transactions are non-fraudulent and \n",
    "100 are fraudulent. In this case, the data is imbalanced because the minority class (fraudulent transactions) \n",
    "has only a small percentage of the total number of samples.\n",
    "\n",
    "If we were to train a machine learning model on this imbalanced dataset, it might be biased towards the\n",
    "majority class and have poor predictive performance for the minority class. To address this issue, we \n",
    "could use up-sampling or down-sampling to balance the class distribution.\n",
    "\n",
    "For example, we could up-sample the minority class by generating synthetic samples using SMOTE, resulting \n",
    "in a new dataset with 900 non-fraudulent transactions and 900 synthetic fraudulent transactions. \n",
    "Alternatively, we could down-sample the majority class by randomly selecting 100 non-fraudulent \n",
    "transactions to create a new dataset with 100 non-fraudulent and 100 fraudulent transactions.\n",
    "\n",
    "By balancing the class distribution through up-sampling or down-sampling, we can improve the\n",
    "performance of the machine learning model for the minority class and ensure that it is not biased \n",
    "towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d265f23-c3bc-4ed2-b26d-aee2602faf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Ans.\n",
    "\n",
    "Data augmentation is a technique used in machine learning to increase the amount \n",
    "and diversity of data available for training models.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a technique used in machine learning \n",
    "to address imbalanced datasets where the minority class has significantly fewer instances \n",
    "than the majority class. SMOTE involves generating synthetic instances of the minority class by \n",
    "interpolating between existing instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1757f-bc97-4947-bdff-a9ebb95c9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Ans.\n",
    "\n",
    "Outliers are data points in a dataset that differ significantly from other\n",
    "observations and can skew the overall statistical analysis and model performance.\n",
    "\n",
    "It is essential to handle outliers in a dataset for several reasons:\n",
    "\n",
    "Outliers can bias the statistical analysis of the data, leading to incorrect conclusions and decisions.\n",
    "\n",
    "Outliers can affect the performance of machine learning models by introducing noise and reducing their \n",
    "accuracy and generalization ability.\n",
    "\n",
    "Outliers can also impact the training process of models, making them more sensitive to noise and\n",
    "leading to overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8cc23-6d50-408b-b2f6-1fa3eeb7f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Ans.\n",
    "\n",
    "There are several techniques you can use to handle missing data in your analysis:\n",
    "\n",
    "Deletion: One option is to simply delete any records with missing data. However, \n",
    "this method can result in a loss of important information and can bias your analysis \n",
    "if the missing data is not missing completely at random.\n",
    "\n",
    "Imputation: Another option is to impute, or estimate, the missing data using a variety of\n",
    "techniques. One of the most common techniques is mean imputation, which involves replacing\n",
    "missing values with the mean value of the non-missing data. Other techniques include regression\n",
    "imputation, hot-deck imputation, and multiple imputation.\n",
    "\n",
    "Data augmentation: This technique involves adding additional data points to your dataset to make \n",
    "up for the missing values. This can be done using various methods, such as generating synthetic \n",
    "data or copying values from similar records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aae74e-6873-4980-b83d-29cabcd97f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Ans.\n",
    "\n",
    "There are several strategies you can use to determine if missing data is \n",
    "missing at random or if there is a pattern to the missing data:\n",
    "    \n",
    "Analyze the distribution of missing data.\n",
    "\n",
    "Use statistical tests: You can use statistical tests to determine if there is a relationship \n",
    "between missing data and other variables in the dataset. For example, you can use a \n",
    "chi-square test to determine if there is a significant relationship between missing data and a \n",
    "categorical variable in the dataset.\n",
    "\n",
    "Visualize the missing data: You can use visualization techniques, such as heatmaps or\n",
    "scatterplots, to visualize the missing data and look for patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2d470-cf0c-4e79-83b7-78377c7be904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Ans.\n",
    "\n",
    "Confusion Matrix: Start by computing the confusion matrix, which is a table that \n",
    "summarizes the number of true positives, false positives, true negatives, and false\n",
    "negatives for your model's predictions. This will help you understand the trade-offs\n",
    "between sensitivity (the ability of the model to correctly identify positive cases) and\n",
    "specificity (the ability of the model to correctly identify negative cases).\n",
    "\n",
    "Precision-Recall Curve: The precision-recall (PR) curve is a graphical representation of\n",
    "the performance of a binary classifier at different classification thresholds.\n",
    "The PR curve can be useful for evaluating classifiers on imbalanced datasets, as it provides a \n",
    "more detailed view of the trade-offs between precision and recall (or sensitivity).\n",
    "\n",
    "F1 Score: F1 score is a harmonic mean of precision and recall, which provides a single score that \n",
    "balances both precision and recall. F1 score is often used as a metric to evaluate the performance \n",
    "of binary classifiers on imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbd841-5f70-4e62-9e81-92479f7b7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.Ans.\n",
    "\n",
    "Here are some approaches that you can try:\n",
    "\n",
    "Undersampling: One approach is to randomly remove samples from the majority class \n",
    "until the dataset is balanced. This method is simple to implement, but it can lead\n",
    "to the loss of important information, especially if the dataset is already small.\n",
    "\n",
    "Oversampling: Another approach is to randomly replicate samples from the minority class\n",
    "until the dataset is balanced. This method increases the sample size of the minority class,\n",
    "but it can also lead to overfitting and poor generalization performance.\n",
    "\n",
    "Synthetic Data Generation: You can generate synthetic data for the minority class using \n",
    "techniques such as SMOTE (Synthetic Minority Over-sampling Technique). This method generates\n",
    "new samples by interpolating between existing minority samples, which can help to increase the \n",
    "size of the minority class without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2272da-c72b-455f-b414-308128ebe961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.Ans.\n",
    "\n",
    "Oversampling: In this method, we randomly duplicate samples from the minority class \n",
    "to increase their numbers. There are different oversampling techniques like\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) that generate synthetic data points to balance the dataset.\n",
    "\n",
    "Undersampling: In this method, we randomly remove samples from the majority class to \n",
    "reduce their numbers. This method is simple and easy to implement, but it can result \n",
    "in loss of information and accuracy.\n",
    "\n",
    "Hybrid methods: These methods combine both oversampling and undersampling techniques to\n",
    "balance the dataset. One such technique is the SMOTEENN method, which first oversamples\n",
    "the minority class using SMOTE and then performs undersampling on the majority class.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
